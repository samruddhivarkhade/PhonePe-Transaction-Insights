{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vncDsAP0Gaoa"
   },
   "source": [
    "# **Project Name - PhonePe Transaction Insights**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beRrZCGUAJYm"
   },
   "source": [
    "##### **Project Type**    - Data Analytics & Visualization Project (Python, MySQL, Streamlit)\n",
    "##### **Contribution**    - Individual\n",
    "##### **Team Member 1  - Samruddhi Jagdish Varkhade**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJNUwmbgGyua"
   },
   "source": [
    "# **Project Summary -**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6v_1wHtG2nS"
   },
   "source": [
    "PhonePe Transaction Insights is a data analytics and visualization project that explores digital payment trends across India using the PhonePe Pulse dataset. The project integrates Python, MySQL, and Streamlit to extract, process, and visualize transaction data through interactive charts and maps. It provides clear insights into transaction volumes, payment modes, user adoption, and geographical patterns, helping understand the digital payment landscape effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6K7xa23Elo4"
   },
   "source": [
    "# **GitHub Link -**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1o69JH3Eqqn"
   },
   "source": [
    "https://github.com/samruddhivarkhade/PhonePe-Transaction-Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQaldy8SH6Dl"
   },
   "source": [
    "# **Problem Statement**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpeJGUA3kjGy"
   },
   "source": [
    "With the increasing reliance on digital payment systems like PhonePe, understanding the dynamics of transactions, user engagement, and insurance-related data is crucial for improving services and targeting users effectively. This project aims to analyze and visualize aggregated values of payment categories, create maps for total values at state and district levels, and identify top-performing states, districts, and pin codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDgbUHAGgjLW"
   },
   "source": [
    "# **General Guidelines** : -  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrxVaUj-hHfC"
   },
   "source": [
    "1.   Well-structured, formatted, and commented code is required.\n",
    "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
    "     \n",
    "     The additional credits will have advantages over other students during Star Student selection.\n",
    "       \n",
    "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
    "                       without a single error logged. ]\n",
    "\n",
    "3.   Each and every logic should have proper comments.\n",
    "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
    "        \n",
    "\n",
    "```\n",
    "# Chart visualization code\n",
    "```\n",
    "            \n",
    "\n",
    "*   Why did you pick the specific chart?\n",
    "*   What is/are the insight(s) found from the chart?\n",
    "* Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason.\n",
    "\n",
    "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
    "\n",
    "\n",
    "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
    "\n",
    "U - Univariate Analysis,\n",
    "\n",
    "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
    "\n",
    "M - Multivariate Analysis\n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
    "\n",
    "\n",
    "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
    "\n",
    "\n",
    "*   Cross- Validation & Hyperparameter Tuning\n",
    "\n",
    "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
    "\n",
    "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_i_v8NEhb9l"
   },
   "source": [
    "# ***Let's Begin !***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhfV-JJviCcP"
   },
   "source": [
    "## ***1. Know Your Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3lxredqlCYt"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M8Vqi-pPk-HR"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 1.1 Import Libraries\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# visualization, and database operations.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ---------------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     10\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Basic data manipulation\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# 1.1 Import Libraries\n",
    "# ---------------------------------------------\n",
    "# Purpose:\n",
    "# Importing all necessary Python libraries for data handling,\n",
    "# visualization, and database operations.\n",
    "# ---------------------------------------------\n",
    "\n",
    "from google.colab import drive # type: ignore\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Basic data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Database connection\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# System and warnings\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RnN4peoiCZX"
   },
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CkvbW_SlZ_R"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 1.2 Load Dataset (Error-Free for Google Colab)\n",
    "# ---------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load dataset directly from Drive\n",
    "file_path = \"/content/drive/MyDrive/PhonePe Transaction Insights/aggregated_transaction.csv\"\n",
    "transactions_df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Data loaded successfully from Google Drive!\")\n",
    "transactions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x71ZqKXriCWQ"
   },
   "source": [
    "### Dataset First View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWNFOSvLl09H"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 1.3 Dataset First View\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(\"Dataset Shape (Rows, Columns):\", transactions_df.shape)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(\"\\n First 5 Rows of the Dataset:\")\n",
    "display(transactions_df.head())\n",
    "\n",
    "# Display column names\n",
    "print(\"\\n Column Names:\")\n",
    "print(transactions_df.columns.tolist())\n",
    "\n",
    "# Check data types of each column\n",
    "print(\"\\n Data Types:\")\n",
    "print(transactions_df.dtypes)\n",
    "\n",
    "# Quick overview of dataset info\n",
    "print(\"\\n Dataset Info:\")\n",
    "transactions_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hBIi_osiCS2"
   },
   "source": [
    "### Dataset Rows & Columns count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kllu7SJgmLij"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 1.4 Dataset Rows and Columns Count\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Total number of rows\n",
    "rows = transactions_df.shape[0]\n",
    "\n",
    "# Total number of columns\n",
    "cols = transactions_df.shape[1]\n",
    "\n",
    "print(\"✅ Total Rows in Dataset:\", rows)\n",
    "print(\"✅ Total Columns in Dataset:\", cols)\n",
    "print(f\"📊 The dataset contains {rows} rows and {cols} columns in total.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlHwYmJAmNHm"
   },
   "source": [
    "### Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9hRXRi6meOf"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "#  Dataset Information\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Display dataset information\n",
    "print(\" Dataset Information:\\n\")\n",
    "transactions_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35m5QtbWiB9F"
   },
   "source": [
    "#### Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sLdpKYkmox0"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# ✅ Safe Duplicate Check and Removal\n",
    "# ---------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "if 'transactions_df' in locals() and not transactions_df.empty:\n",
    "    # Convert unhashable columns (list/dict) to string temporarily\n",
    "    df_temp = transactions_df.copy()\n",
    "    for col in df_temp.columns:\n",
    "        df_temp[col] = df_temp[col].apply(lambda x: str(x) if isinstance(x, (list, dict)) else x)\n",
    "\n",
    "    # Step 1: Count duplicate rows safely\n",
    "    duplicate_count = df_temp.duplicated().sum()\n",
    "    print(f\"🔍 Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "    # Step 2: Remove duplicates safely\n",
    "    if duplicate_count > 0:\n",
    "        transactions_df = transactions_df.loc[~df_temp.duplicated()].reset_index(drop=True)\n",
    "        print(f\"✅ Duplicate rows removed successfully! Remaining rows: {len(transactions_df)}\")\n",
    "    else:\n",
    "        print(\"✅ No duplicate rows found.\")\n",
    "else:\n",
    "    print(\"⚠️ DataFrame 'transactions_df' not found or is empty. Please load your dataset first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoPl-ycgm1ru"
   },
   "source": [
    "#### Missing Values/Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgHWkxvamxVg"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "#  Missing Values / Null Values Count\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Count missing values for each column\n",
    "missing_values = transactions_df.isnull().sum()\n",
    "\n",
    "print(\"🔍 Missing / Null Values Count:\\n\")\n",
    "print(missing_values)\n",
    "\n",
    "# Optional: Show only columns with missing data\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "if len(missing_columns) > 0:\n",
    "    print(\"\\n⚠️ Columns with missing values:\")\n",
    "    print(missing_columns)\n",
    "else:\n",
    "    print(\"\\n✅ No missing values found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3q5wnI3om9sJ"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "#  Visualizing Missing Values\n",
    "# ---------------------------------------------\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(transactions_df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"🔍 Missing Values Heatmap\", fontsize=16)\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Rows\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0kj-8xxnORC"
   },
   "source": [
    "### What did you know about your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfoNAAC-nUe_"
   },
   "source": [
    "The dataset contains information about PhonePe transactions across different states, years, and quarters. It includes details such as the transaction type, count, and amount, helping us understand how digital payments vary across regions and over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA9Y7ga8ng1Z"
   },
   "source": [
    "## ***2. Understanding Your Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7xfkqrt5Ag5"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "#  Dataset Columns\n",
    "# ---------------------------------------------\n",
    "# Display all column names in the dataset\n",
    "print(\"📋 Dataset Columns:\\n\")\n",
    "print(transactions_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnOaZdaE5Q5t"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Dataset Description\n",
    "# ---------------------------------------------\n",
    "# Display statistical summary of numerical columns\n",
    "print(\"📊 Dataset Description:\\n\")\n",
    "transactions_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBTbrJXOngz2"
   },
   "source": [
    "### Variables Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJV4KIxSnxay"
   },
   "source": [
    "State: Name of the state.\n",
    "\n",
    "\n",
    "Year: Year of transaction.\n",
    "\n",
    "\n",
    "Quarter: Quarter (Q1–Q4).\n",
    "\n",
    "\n",
    "Transaction_type: Type of transaction.\n",
    "\n",
    "\n",
    "Transaction_count: Number of transactions.\n",
    "\n",
    "\n",
    "Transaction_amount: Total transaction value (INR).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3PMJOP6ngxN"
   },
   "source": [
    "### Check Unique Values for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zms12Yq5n-jE"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 2.4 Check Unique Values for Each Variable\n",
    "# ---------------------------------------------\n",
    "for column in transactions_df.columns:\n",
    "    unique_count = transactions_df[column].nunique()\n",
    "    print(f\"{column}: {unique_count} unique values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dauF4eBmngu3"
   },
   "source": [
    "## 3. ***Data Wrangling***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKJF3rekwFvQ"
   },
   "source": [
    "### Data Wrangling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk-9a2fpoLcV"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "#  Data Cleaning - Make Dataset Analysis Ready\n",
    "# ---------------------------------------------\n",
    "\n",
    "# 1. Remove duplicate rows (if any)\n",
    "transactions_df = transactions_df.drop_duplicates()\n",
    "\n",
    "# 2. Handle missing values — fill or remove based on requirement\n",
    "transactions_df = transactions_df.dropna()  # removing missing rows for clean analysis\n",
    "\n",
    "# 3. Convert column names to lowercase for consistency\n",
    "transactions_df.columns = transactions_df.columns.str.lower()\n",
    "\n",
    "# 4. Remove leading/trailing spaces from string columns\n",
    "transactions_df = transactions_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# 5. Verify dataset after cleaning\n",
    "print(\"✅ Dataset is now clean and analysis-ready!\")\n",
    "transactions_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSa1f5Uengrz"
   },
   "source": [
    "### What all manipulations have you done and insights you found?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbyXE7I1olp8"
   },
   "source": [
    "**Manipulations Done:**\n",
    "\n",
    "Removed duplicate records to ensure data consistency.\n",
    "\n",
    "Dropped missing/null values for accurate analysis.\n",
    "\n",
    "Standardized column names to lowercase for uniformity.\n",
    "\n",
    "Removed extra spaces from text fields for clean formatting.\n",
    "\n",
    "Verified data types and structure for analysis readiness.\n",
    "\n",
    "**Insights Found:**\n",
    "\n",
    "The dataset contains transaction details across different states and years.\n",
    "\n",
    "No major data quality issues were found after cleaning.\n",
    "\n",
    "The dataset is now well-structured and ready for visualization and deeper analysis (univariate, bivariate, multivariate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GF8Ens_Soomf"
   },
   "source": [
    "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wOQAZs5pc--"
   },
   "source": [
    "#### Chart - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v_ESjsspbW7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group data by year\n",
    "yearly_data = transactions_df.groupby('year')['transaction_count'].sum().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(yearly_data['year'], yearly_data['transaction_count'], marker='o', color='purple')\n",
    "plt.title('Total Transactions by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5QZ13OEpz2H"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XESiWehPqBRc"
   },
   "source": [
    "I chose the line chart because it clearly shows the trend of transactions over the years.\n",
    "\n",
    "It helps visualize how the total number of transactions has increased or decreased with time, making it easy to identify growth patterns, seasonal variations, or sudden changes in user activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQ7QKXXCp7Bj"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_j1G7yiqdRP"
   },
   "source": [
    "The insight from the chart is that the total number of transactions has shown a steady increase over the years, indicating growing digital payment adoption and higher user engagement with PhonePe over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "448CDAPjqfQr"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cspy4FjqxJW"
   },
   "source": [
    "Yes , the gained insights can help create a **positive business impact** because the **steady growth in transaction count** shows increasing user trust and market expansion — valuable for attracting investors and designing new financial services.\n",
    "\n",
    "No , there are **no signs of negative growth**, as the transaction trend consistently increases every year, indicating **healthy user retention and growing digital adoption** without any major decline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSlN3yHqYklG"
   },
   "source": [
    "#### Chart - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4YgtaqtYklH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Grouping data by state\n",
    "statewise_amount = transactions_df.groupby('state')['transaction_amount'].sum().reset_index()\n",
    "\n",
    "# Sort the data for better visualization\n",
    "statewise_amount = statewise_amount.sort_values(by='transaction_amount', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=statewise_amount, x='state', y='transaction_amount', palette='viridis')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Total Transaction Amount by State', fontsize=14)\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Transaction Amount (in billions)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6dVpIINYklI"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aaW0BYyYklI"
   },
   "source": [
    "I chose a bar chart because it’s ideal for comparing transaction amounts across different states and visually highlights which states contribute most to total transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijmpgYnKYklI"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSx9atu2YklI"
   },
   "source": [
    "The chart shows that economically developed states like Maharashtra, Karnataka, and Delhi have the highest transaction amounts, indicating stronger digital payment adoption and user activity in these regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JiQyfWJYklI"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcBbebzrYklV"
   },
   "source": [
    "Yes , these insights help identify high-performing regions for targeted marketing, partnerships, and expansion strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM7whBJCYoAo"
   },
   "source": [
    "#### Chart - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6GMdE67YoAp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Grouping data by transaction type\n",
    "typewise_count = transactions_df.groupby('transaction_type')['transaction_count'].sum().reset_index()\n",
    "\n",
    "# Sort for clear visualization\n",
    "typewise_count = typewise_count.sort_values(by='transaction_count', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=typewise_count, x='transaction_type', y='transaction_count', palette='magma')\n",
    "plt.title('Total Transaction Count by Transaction Type', fontsize=14)\n",
    "plt.xlabel('Transaction Type')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fge-S5ZAYoAp"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dBItgRVYoAp"
   },
   "source": [
    "A bar chart was chosen because it effectively compares different categories of transaction types, making it easy to see which type dominates in terms of count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85gYPyotYoAp"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jstXR6OYoAp"
   },
   "source": [
    "The chart shows that Peer-to-Peer (P2P) and Recharge/Bill Payments transactions have the highest counts, suggesting that users frequently use digital platforms for everyday transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoGjAbkUYoAp"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfJ8IqMcYoAp"
   },
   "source": [
    "Yes, These insights help digital payment companies focus on improving UX and offers around the most-used services (like P2P transfers and bill payments).\n",
    "\n",
    "There are no signs of negative growth, but low transaction types (like merchant payments in smaller regions) may need awareness campaigns or cashback offers to boost adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Of9eVA-YrdM"
   },
   "source": [
    "#### Chart - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irlUoxc8YrdO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Group data by year and calculate total transaction amount\n",
    "yearly_amount = transactions_df.groupby('year')['transaction_amount'].sum().reset_index()\n",
    "\n",
    "# Plot line chart\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.lineplot(data=yearly_amount, x='year', y='transaction_amount', marker='o', color='purple')\n",
    "plt.title('Yearly Transaction Amount Trend', fontsize=14)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Transaction Amount')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iky9q4vBYrdO"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJRCwT6DYrdO"
   },
   "source": [
    "A line chart was chosen because it clearly shows how the transaction amount changes over time (yearly trend). It’s ideal for analyzing growth and time-based variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6T5p64dYrdO"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xx8WAJvtYrdO"
   },
   "source": [
    "The chart indicates a consistent increase in total transaction amount year by year, showing that digital payment adoption is growing rapidly across the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-Ehk30pYrdP"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLNxxz7MYrdP"
   },
   "source": [
    "Yes, The upward trend confirms that digital transactions are becoming a core part of the economy, encouraging businesses to integrate digital payment gateways and offer cashless incentives.\n",
    "No major negative insights are found — the growth trend reflects positive user trust and market expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bamQiAODYuh1"
   },
   "source": [
    "#### Chart - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIJwrbroYuh3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate average transaction amount per transaction type\n",
    "avg_transaction_type = transactions_df.groupby('transaction_type')['transaction_amount'].mean().reset_index()\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(data=avg_transaction_type, x='transaction_type', y='transaction_amount', palette='viridis')\n",
    "plt.title('Average Transaction Amount by Transaction Type', fontsize=14)\n",
    "plt.xlabel('Transaction Type')\n",
    "plt.ylabel('Average Transaction Amount')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHF8YVU7Yuh3"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcxuIMRPYuh3"
   },
   "source": [
    "A bar chart was chosen because it effectively compares average transaction amounts across different transaction types, making it easy to spot which types have higher monetary values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwzvFGzlYuh3"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyqkiB8YYuh3"
   },
   "source": [
    "Certain transaction types such as merchant payments and peer-to-peer transfers show higher average amounts, indicating these categories drive most of the financial volume in digital transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYpmQ266Yuh3"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WtzZ_hCYuh4"
   },
   "source": [
    "Yes. Understanding which transaction types handle larger amounts helps PhonePe and similar platforms prioritize improvements, marketing, and partnerships in those areas.\n",
    "No negative impact observed — but lower-value categories might need better incentives or user education to increase engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OH-pJp9IphqM"
   },
   "source": [
    "#### Chart - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuRf4wtuphqN"
   },
   "outputs": [],
   "source": [
    "# Group and sort data by total transaction amount\n",
    "top_states = transactions_df.groupby('state')['transaction_amount'].sum().reset_index().sort_values(by='transaction_amount', ascending=False).head(10)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=top_states, x='transaction_amount', y='state', palette='mako')\n",
    "plt.title('Top 10 States by Total Transaction Amount', fontsize=14)\n",
    "plt.xlabel('Total Transaction Amount')\n",
    "plt.ylabel('State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbFf2-_FphqN"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loh7H2nzphqN"
   },
   "source": [
    "A horizontal bar chart was chosen to easily compare large numerical values (transaction amounts) across multiple states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ouA3fa0phqN"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VECbqPI7phqN"
   },
   "source": [
    "States like Maharashtra, Karnataka, and Tamil Nadu lead in transaction volume, indicating higher digital payment adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Seke61FWphqN"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW4_bGpfphqN"
   },
   "source": [
    "Positive — shows regions with strong digital payment penetration for targeted business growth.\n",
    "Other states with low activity indicate areas where awareness campaigns can boost usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIIx-8_IphqN"
   },
   "source": [
    "#### Chart - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqAIGUfyphqO"
   },
   "outputs": [],
   "source": [
    "# Group by year and quarter\n",
    "year_quarter = transactions_df.groupby(['year', 'quarter'])['transaction_count'].sum().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=year_quarter, x='quarter', y='transaction_count', hue='year', marker='o', palette='Set2')\n",
    "plt.title('Total Transaction Count by Year and Quarter', fontsize=14)\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t27r6nlMphqO"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv6ro40sphqO"
   },
   "source": [
    "A line chart effectively shows temporal trends and comparisons across years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2jJGEOYphqO"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po6ZPi4hphqO"
   },
   "source": [
    "There’s a clear upward trend each year, with transactions peaking in Q4 — likely due to festive and shopping seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0JNsNcRphqO"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvSq8iUTphqO"
   },
   "source": [
    "Positive — indicates strong user engagement and seasonal transaction growth.\n",
    "Can help businesses plan promotional campaigns during high-transaction periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZR9WyysphqO"
   },
   "source": [
    "#### Chart - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdPTWpAVphqO"
   },
   "outputs": [],
   "source": [
    "# Scatter plot for correlation\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(data=transactions_df, x='transaction_count', y='transaction_amount', alpha=0.6)\n",
    "plt.title('Correlation Between Transaction Count and Transaction Amount', fontsize=14)\n",
    "plt.xlabel('Transaction Count')\n",
    "plt.ylabel('Transaction Amount')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jj7wYXLtphqO"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob8u6rCTphqO"
   },
   "source": [
    "A scatter plot is ideal for identifying relationships between two continuous variables — in this case, transaction count and amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZrbJ2SmphqO"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZtgC_hjphqO"
   },
   "source": [
    "A positive correlation exists: as transaction counts increase, total transaction amounts also rise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFu4xreNphqO"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ey_0qi68phqO"
   },
   "source": [
    "Positive — suggests that higher engagement directly drives revenue growth.\n",
    "Encouraging users to make more transactions could significantly boost overall volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ55k-q6phqO"
   },
   "source": [
    "#### Chart - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2aS4O1ophqO"
   },
   "outputs": [],
   "source": [
    "# Grouping by transaction type\n",
    "avg_amount_type = transactions_df.groupby('transaction_type')['transaction_amount'].mean().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=avg_amount_type, x='transaction_type', y='transaction_amount', palette='crest')\n",
    "plt.title('Average Transaction Amount by Transaction Type', fontsize=14)\n",
    "plt.xlabel('Transaction Type')\n",
    "plt.ylabel('Average Transaction Amount')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCFgpxoyphqP"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVxDimi2phqP"
   },
   "source": [
    "A bar chart clearly shows how the average transaction amount changes over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVtJsKN_phqQ"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngGi97qjphqQ"
   },
   "source": [
    "There’s a consistent increase in the average transaction amount, showing growing trust and comfort in making larger digital payments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lssrdh5qphqQ"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBpY5ekJphqQ"
   },
   "source": [
    "Positive — suggests users are transacting more confidently online.\n",
    "Indicates economic growth and stronger consumer spending behavior in digital payments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2RJ9gkRphqQ"
   },
   "source": [
    "#### Chart - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GM7a4YP4phqQ"
   },
   "outputs": [],
   "source": [
    "# Group by transaction type\n",
    "txn_type = transactions_df.groupby('transaction_type')['transaction_amount'].sum().reset_index().sort_values(by='transaction_amount', ascending=False).head(5)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=txn_type, x='transaction_type', y='transaction_amount', palette='viridis')\n",
    "plt.title('Top 5 Transaction Types by Total Amount', fontsize=14)\n",
    "plt.xlabel('Transaction Type')\n",
    "plt.ylabel('Total Transaction Amount')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M8mcRywphqQ"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8agQvks0phqQ"
   },
   "source": [
    "A bar chart helps to compare contribution of different transaction types to the total payment volume.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgIPom80phqQ"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp13pnNzphqQ"
   },
   "source": [
    "Peer-to-peer (P2P) and merchant payments dominate the transaction categories, showing that daily use and commercial transactions drive digital adoption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMzcOPDDphqR"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4Ka1PC2phqR"
   },
   "source": [
    "Positive — helps identify which services (like UPI, merchant payments) generate the most value.\n",
    "Businesses can focus on strengthening high-usage categories and promoting underperforming ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-EpHcCOp1ci"
   },
   "source": [
    "#### Chart - 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAQTIvtqp1cj"
   },
   "outputs": [],
   "source": [
    "# Group by year and quarter\n",
    "year_quarter_txn = transactions_df.groupby(['year', 'quarter'])['transaction_amount'].sum().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=year_quarter_txn, x='quarter', y='transaction_amount', hue='year', palette='plasma')\n",
    "plt.title('Yearly Transaction Amount by Quarter', fontsize=14)\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Transaction Amount')\n",
    "plt.legend(title='Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_VqEhTip1ck"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vsMzt_np1ck"
   },
   "source": [
    "A grouped bar chart is ideal for comparing transaction patterns across quarters within each year, helping spot seasonal trends and yearly growth together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zGJKyg5p1ck"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYdMsrqVp1ck"
   },
   "source": [
    "There’s a steady increase in transaction volume across all quarters each year, with Q4 showing peak activity consistently. This suggests year-end festive and shopping seasons drive more digital transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVzmfK_Ep1ck"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "druuKYZpp1ck"
   },
   "source": [
    "Positive impact: Identifies high-activity quarters where campaigns or merchant tie-ups can maximize engagement.\n",
    "Shows consistent growth across all quarters, proving users’ increasing trust in digital payments.\n",
    "Helps businesses allocate resources efficiently during expected transaction peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3dbpmDWp1ck"
   },
   "source": [
    "#### Chart - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwevp1tKp1ck"
   },
   "outputs": [],
   "source": [
    "# Group by quarter\n",
    "quarterly_txn = transactions_df.groupby('quarter')['transaction_count'].sum().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=quarterly_txn, x='quarter', y='transaction_count', palette='mako')\n",
    "plt.title('Total Transactions by Quarter (All Years)', fontsize=14)\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Total Transaction Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylSl6qgtp1ck"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2xqNkiQp1ck"
   },
   "source": [
    "A bar chart effectively highlights differences in total transactions across quarters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWILFDl5p1ck"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-lUsV2mp1ck"
   },
   "source": [
    "Quarter 4 shows the highest transactions each year — likely due to festive seasons and year-end shopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7G43BXep1ck"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wwDJXsLp1cl"
   },
   "source": [
    "Positive — shows peak periods where digital activity surges.\n",
    "Businesses can align marketing or cashback offers during high-transaction quarters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag9LCva-p1cl"
   },
   "source": [
    "#### Chart - 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUfxeq9-p1cl"
   },
   "outputs": [],
   "source": [
    "# Group by state\n",
    "state_avg_txn = transactions_df.groupby('state')['transaction_count'].mean().reset_index().sort_values(by='transaction_count', ascending=False).head(10)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=state_avg_txn, x='transaction_count', y='state', palette='cubehelix')\n",
    "plt.title('Top 10 States by Average Transaction Count', fontsize=14)\n",
    "plt.xlabel('Average Transaction Count')\n",
    "plt.ylabel('State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6MkPsBcp1cl"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V22bRsFWp1cl"
   },
   "source": [
    "A horizontal bar chart provides a clear visual comparison among states’ average digital activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cELzS2fp1cl"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozQPc2_Ip1cl"
   },
   "source": [
    "States like Karnataka, Maharashtra, and Delhi consistently perform better, indicating strong user engagement and digital literacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MPXvC8up1cl"
   },
   "source": [
    "##### 3. Will the gained insights help creating a positive business impact?\n",
    "Are there any insights that lead to negative growth? Justify with specific reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GL8l1tdLp1cl"
   },
   "source": [
    "Positive — highlights key states leading digital payment adoption.\n",
    "Lower-performing regions indicate opportunities for awareness programs and business expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC_X3p0fY2L0"
   },
   "source": [
    "#### Chart - 14 - Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyC9zolEZNRQ"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select only numerical columns for correlation\n",
    "numeric_df = transactions_df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='Purples', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap of Numerical Variables', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV0SzAkaZNRQ"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVPuT8LYZNRQ"
   },
   "source": [
    "A correlation heatmap helps identify relationships between numerical features like transaction_count, transaction_amount, and year.\n",
    "It’s useful for quickly spotting which factors influence each other most strongly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPEH6qLeZNRQ"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfSqtnDqZNRR"
   },
   "source": [
    "There’s a strong positive correlation between transaction_count and transaction_amount, meaning higher transaction counts usually result in higher total transaction value.\n",
    "\n",
    "year also shows a mild positive correlation, indicating a year-over-year growth trend in digital payments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q29F0dvdveiT"
   },
   "source": [
    "#### Chart - 15 - Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o58-TEIhveiU"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pairplot to visualize relationships between numerical features\n",
    "sns.pairplot(transactions_df, vars=['year', 'transaction_count', 'transaction_amount'], hue='state', palette='viridis')\n",
    "plt.suptitle('Pair Plot: Relationships Between Key Numerical Variables', y=1.02, fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXh0U9oCveiU"
   },
   "source": [
    "##### 1. Why did you pick the specific chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMmPjTByveiU"
   },
   "source": [
    "The pair plot provides a quick and comprehensive view of how multiple numerical variables relate to each other — both individually (through histograms) and pairwise (through scatter plots).\n",
    "It helps detect patterns, trends, and outliers easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22aHeOlLveiV"
   },
   "source": [
    "##### 2. What is/are the insight(s) found from the chart?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPQ8RGwHveiV"
   },
   "source": [
    "transaction_count and transaction_amount show a clear linear relationship — more transactions lead to higher transaction value.\n",
    "\n",
    "The distribution across years shows a consistent upward growth in both transaction volume and amount.\n",
    "\n",
    "Certain states stand out as high-performing regions based on higher transaction metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ATYxFrGrvw"
   },
   "source": [
    "## ***5. Hypothesis Testing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yfr_Vlr8HBkt"
   },
   "source": [
    "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7MS06SUHkB-"
   },
   "source": [
    "Here are three well-defined hypothetical statements (H₀ & H₁) that we can test based on your dataset — especially since it includes state, year, transaction_count, and transaction_amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yEUt7NnHlrM"
   },
   "source": [
    "### Hypothetical Statement - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEA2Xm5dHt1r"
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI9ZP0laH0D-"
   },
   "source": [
    "Research Statement:\n",
    "The average transaction amount significantly varies between different states.\n",
    "\n",
    "Null Hypothesis (H₀): There is no significant difference in the mean transaction amount across different states.\n",
    "\n",
    "Alternate Hypothesis (H₁): There is a significant difference in the mean transaction amount across different states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I79__PHVH19G"
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZrfquKtyian"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Group data by state\n",
    "state_groups = [group[\"transaction_amount\"].values for name, group in transactions_df.groupby(\"state\")]\n",
    "\n",
    "# Perform one-way ANOVA test\n",
    "anova_result = stats.f_oneway(*state_groups)\n",
    "print(\"F-statistic:\", anova_result.statistic)\n",
    "print(\"p-value:\", anova_result.pvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou-I18pAyIpj"
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2U0kk00ygSB"
   },
   "source": [
    "Statistical Test Used: One-Way ANOVA (Analysis of Variance)\n",
    "\n",
    "Purpose: To compare the mean transaction amounts across more than two groups (states).\n",
    "\n",
    "P-value meaning:\n",
    "If p < 0.05, it means at least one state’s average transaction amount is significantly different from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fF3858GYyt-u"
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO4K0gP5y3B4"
   },
   "source": [
    "The variable “state” is categorical (multiple groups).\n",
    "\n",
    "The variable “transaction_amount” is numerical (continuous).\n",
    "\n",
    "You want to compare means across more than two groups (different states).\n",
    "✅ ANOVA is ideal for testing whether the means of several groups are significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_0_7-oCpUZd"
   },
   "source": [
    "### Hypothetical Statement - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwyV_J3ipUZe"
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnpLGJ-4pUZe"
   },
   "source": [
    "Research Statement:\n",
    "There is a positive correlation between transaction count and transaction amount.\n",
    "\n",
    "Null Hypothesis (H₀): There is no correlation between transaction count and transaction amount.\n",
    "\n",
    "Alternate Hypothesis (H₁): There is a positive correlation between transaction count and transaction amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yB-zSqbpUZe"
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWxdNTXNpUZe"
   },
   "outputs": [],
   "source": [
    "# Perform Pearson correlation test\n",
    "corr_coeff, p_value = stats.pearsonr(transactions_df[\"transaction_count\"], transactions_df[\"transaction_amount\"])\n",
    "print(\"Correlation Coefficient:\", corr_coeff)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEUvejAfpUZe"
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLDrPz7HpUZf"
   },
   "source": [
    "Statistical Test Used: Pearson Correlation Test\n",
    "\n",
    "Purpose: To measure the strength and direction of a linear relationship between two continuous variables.\n",
    "\n",
    "P-value meaning:\n",
    "If p < 0.05, the correlation is statistically significant — meaning the relationship is unlikely due to random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd15vwWVpUZf"
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xOGYyiBpUZf"
   },
   "source": [
    "Reason for Choosing:\n",
    "\n",
    "Both “transaction_count” and “transaction_amount” are continuous numerical variables.\n",
    "\n",
    "You want to test the linear relationship between them.\n",
    " Pearson’s correlation coefficient (r) quantifies how strongly two continuous variables are linearly related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn_IUdTipZyH"
   },
   "source": [
    "### Hypothetical Statement - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49K5P_iCpZyH"
   },
   "source": [
    "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gWI5rT9pZyH"
   },
   "source": [
    "Null Hypothesis (H₀):\n",
    "The average transaction amount in 2022 is equal to or less than that of 2021.\n",
    "\n",
    "Alternative Hypothesis (H₁):\n",
    "The average transaction amount in 2022 is significantly higher than that of 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nff-vKELpZyI"
   },
   "source": [
    "#### 2. Perform an appropriate statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6AnJQjtpZyI"
   },
   "outputs": [],
   "source": [
    "# Group data by quarter\n",
    "quarter_groups = [group[\"transaction_amount\"].values for name, group in transactions_df.groupby(\"quarter\")]\n",
    "\n",
    "# Perform ANOVA\n",
    "anova_quarter = stats.f_oneway(*quarter_groups)\n",
    "print(\"F-statistic:\", anova_quarter.statistic)\n",
    "print(\"p-value:\", anova_quarter.pvalue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLW572S8pZyI"
   },
   "source": [
    "##### Which statistical test have you done to obtain P-Value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytWJ8v15pZyI"
   },
   "source": [
    "Statistical Test Used: One-Way ANOVA\n",
    "\n",
    "Purpose: To compare the mean transaction amount across multiple quarters (Q1, Q2, Q3, Q4).\n",
    "\n",
    "P-value meaning:\n",
    "If p < 0.05, at least one quarter’s mean differs significantly from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWbDXHzopZyI"
   },
   "source": [
    "##### Why did you choose the specific statistical test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M99G98V6pZyI"
   },
   "source": [
    "The variable “quarter” is categorical (4 groups → Q1, Q2, Q3, Q4).\n",
    "\n",
    "The variable “transaction_amount” is continuous.\n",
    "\n",
    "You want to test whether average amounts differ across time periods.\n",
    " Again, ANOVA is best suited for comparing means of a numerical variable across multiple categorical groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLjJCtPM0KBk"
   },
   "source": [
    "## ***6. Feature Engineering & Data Pre-processing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiyOF9F70UgQ"
   },
   "source": [
    "### 1. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRsAHk1K0fpS"
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "#  Handling Missing Values & Imputation\n",
    "# ----------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample Dataset (Replace with your own)\n",
    "data = {\n",
    "    'Age': [25, 30, np.nan, 22, 28, np.nan, 35],\n",
    "    'Gender': ['Male', 'Female', np.nan, 'Female', 'Male', 'Male', np.nan],\n",
    "    'Income': [50000, 60000, 55000, np.nan, 52000, 58000, np.nan],\n",
    "    'City': ['Pune', np.nan, 'Mumbai', 'Delhi', 'Pune', 'Delhi', np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"🔹 Original Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Check Missing Values\n",
    "# --------------------------------------------\n",
    "print(\"\\n🔍 Missing Value Count per Column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# --------------------------------------------\n",
    "# Visualize Missing Values\n",
    "# --------------------------------------------\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Missing Values Visualization\")\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------\n",
    "# Handle Missing Values\n",
    "# --------------------------------------------\n",
    "\n",
    "# Numerical Columns Imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='mean')  # Options: mean, median, most_frequent\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Separate Columns\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply Imputation\n",
    "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "# --------------------------------------------\n",
    "# Verify After Imputation\n",
    "# --------------------------------------------\n",
    "print(\"\\n✅ Dataset After Imputation:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\n🔍 Missing Values After Imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wuGOrhz0itI"
   },
   "source": [
    "#### What all missing value imputation techniques have you used and why did you use those techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ixusLtI0pqI"
   },
   "source": [
    "I used **mean imputation** for numerical columns to replace missing values with the average, keeping the dataset balanced and avoiding data loss.\n",
    "For categorical columns, I used **most frequent (mode) imputation** to fill missing values with the most common category, maintaining consistency.\n",
    "These techniques are simple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id1riN9m0vUs"
   },
   "source": [
    "### 2. Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6w2CzZf04JK"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Handling Outliers & Outlier Treatments\n",
    "# ---------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize outliers using boxplots\n",
    "numeric_cols = transactions_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sns.boxplot(data=transactions_df, x=col)\n",
    "    plt.title(f'Boxplot for {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Outlier treatment using IQR method\n",
    "for col in numeric_cols:\n",
    "    Q1 = transactions_df[col].quantile(0.25)\n",
    "    Q3 = transactions_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Cap outliers to lower and upper limits\n",
    "    transactions_df[col] = np.where(transactions_df[col] < lower_limit, lower_limit,\n",
    "                             np.where(transactions_df[col] > upper_limit, upper_limit, transactions_df[col]))\n",
    "\n",
    "print(\"✅ Outliers handled successfully using IQR capping method.\")\n",
    "transactions_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "578E2V7j08f6"
   },
   "source": [
    "##### What all outlier treatment techniques have you used and why did you use those techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGZz5OrT1HH-"
   },
   "source": [
    "I used the **IQR (Interquartile Range) method** to detect and treat outliers.\n",
    "This technique identifies extreme values that fall outside 1.5 times the IQR below Q1 or above Q3.\n",
    "I chose it because it’s a **robust, simple, and effective** method that minimizes the influence of extreme values while preserving the overall data distribution and accuracy for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89xtkJwZ18nB"
   },
   "source": [
    "### 3. Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21JmIYMG2hEo"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Encode Categorical Columns\n",
    "# ---------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a copy to avoid modifying original data\n",
    "encoded_df = transactions_df.copy()\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = encoded_df.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical Columns:\", list(categorical_cols))\n",
    "\n",
    "# Apply Label Encoding\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    encoded_df[col] = le.fit_transform(encoded_df[col])\n",
    "\n",
    "print(\"\\n✅ Categorical columns encoded successfully!\")\n",
    "encoded_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67NQN5KX2AMe"
   },
   "source": [
    "#### What all categorical encoding techniques have you used & why did you use those techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDaue5h32n_G"
   },
   "source": [
    "I used **Label Encoding** to convert categorical columns (like state and transaction_type) into numeric form.\n",
    "This method was chosen because:\n",
    "\n",
    "* It is **simple and efficient** for algorithms that can handle ordinal numeric values.\n",
    "* The dataset has **no high-cardinality categorical features**, so Label Encoding works well.\n",
    "* It helps make the data **ML-model ready** without increasing dimensionality (unlike One-Hot Encoding).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iwf50b-R2tYG"
   },
   "source": [
    "### 4. Textual Data Preprocessing\n",
    "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMQiZwjn3iu7"
   },
   "source": [
    "#### 1. Expand Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTouz10C3oNN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVIkgGqN3qsr"
   },
   "source": [
    "#### 2. Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88JnJ1jN3w7j"
   },
   "outputs": [],
   "source": [
    "transactions_df['state'] = transactions_df['state'].str.lower()\n",
    "transactions_df['transaction_type'] = transactions_df['transaction_type'].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkPnILGE3zoT"
   },
   "source": [
    "#### 3. Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqbBqNaA33c0"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Remove Punctuations (if any)\n",
    "# ---------------------------------------------\n",
    "import string\n",
    "\n",
    "# Define columns where text cleaning might be useful\n",
    "text_columns = ['state', 'transaction_type']\n",
    "\n",
    "# Remove punctuation\n",
    "for col in text_columns:\n",
    "    transactions_df[col] = transactions_df[col].astype(str).apply(\n",
    "        lambda x: x.translate(str.maketrans('', '', string.punctuation))\n",
    "    )\n",
    "\n",
    "print(\"✅ Punctuation removed successfully (if any)!\")\n",
    "transactions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hlsf0x5436Go"
   },
   "source": [
    "#### 4. Removing URLs & Removing words and digits contain digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sxKgKxu4Ip3"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Remove URLs & Words Containing Digits\n",
    "# ---------------------------------------------\n",
    "import re\n",
    "\n",
    "# Define columns that may contain text data\n",
    "text_columns = ['state', 'transaction_type']\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove URLs (http, https, www)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove words containing digits\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "for col in text_columns:\n",
    "    transactions_df[col] = transactions_df[col].astype(str).apply(clean_text)\n",
    "\n",
    "print(\"✅ URLs and words with digits removed successfully!\")\n",
    "transactions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT9DMSJo4nBL"
   },
   "source": [
    "#### 5. Removing Stopwords & Removing White spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2LSJh154s8W"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Remove Stopwords\n",
    "# ---------------------------------------------\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords (only the first time)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define columns that may contain text data\n",
    "text_columns = ['state', 'transaction_type']\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "for col in text_columns:\n",
    "    transactions_df[col] = transactions_df[col].astype(str).apply(remove_stopwords)\n",
    "\n",
    "print(\"✅ Stopwords removed successfully!\")\n",
    "transactions_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgLJGffy4vm0"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Remove White Spaces\n",
    "# ---------------------------------------------\n",
    "# Function to strip and clean extra spaces\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Apply to all text-based columns\n",
    "text_columns = ['state', 'transaction_type']\n",
    "\n",
    "for col in text_columns:\n",
    "    transactions_df[col] = transactions_df[col].astype(str).apply(remove_whitespace)\n",
    "\n",
    "print(\"✅ Extra white spaces removed successfully!\")\n",
    "transactions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c49ITxTc407N"
   },
   "source": [
    "#### 6. Rephrase Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foqY80Qu48N2"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Remove White Spaces\n",
    "# ---------------------------------------------\n",
    "# Function to strip and clean extra spaces\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Apply to all text-based columns\n",
    "text_columns = ['state', 'transaction_type']\n",
    "\n",
    "for col in text_columns:\n",
    "    transactions_df[col] = transactions_df[col].astype(str).apply(remove_whitespace)\n",
    "\n",
    "print(\"✅ Extra white spaces removed successfully!\")\n",
    "transactions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeJFEK0N496M"
   },
   "source": [
    "#### 7. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijx1rUOS5CUU"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Tokenization (Fixed version)\n",
    "# ---------------------------------------------\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required tokenizers\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Example text column — using 'transaction_type' for demonstration\n",
    "transactions_df['tokens'] = transactions_df['transaction_type'].astype(str).apply(word_tokenize)\n",
    "\n",
    "print(\"✅ Text successfully tokenized into individual words!\")\n",
    "transactions_df[['transaction_type', 'tokens']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ExmJH0g5HBk"
   },
   "source": [
    "#### 8. Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIJ1a-Zc5PY8"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Normalizing Text (Stemming & Lemmatization)\n",
    "# ---------------------------------------------\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply both Stemming and Lemmatization\n",
    "transactions_df['stemmed'] = transactions_df['transaction_type'].astype(str).apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "transactions_df['lemmatized'] = transactions_df['transaction_type'].astype(str).apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "print(\"✅ Text normalization completed successfully (Stemming & Lemmatization applied)!\")\n",
    "transactions_df[['transaction_type', 'stemmed', 'lemmatized']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJNqERVU536h"
   },
   "source": [
    "##### Which text normalization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9jKVxE06BC1"
   },
   "source": [
    "Answer Here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5UmGsbsOxih"
   },
   "source": [
    "#### 9. Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btT3ZJBAO6Ik"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# POS Tagging (Fixed for Google Colab)\n",
    "# ---------------------------------------------\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download all necessary resources safely\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # Updated model name for newer NLTK versions\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing helps computers understand human language.\"\n",
    "\n",
    "# Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Display results\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"\\nPOS Tags:\")\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word} → {tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0VqWOYE6DLQ"
   },
   "source": [
    "#### 10. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBRtdhth6JDE"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example text data\n",
    "texts = transactions_df['transaction_type'].astype(str)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=500)\n",
    "\n",
    "# Transform text into numeric vectors\n",
    "tfidf_matrix = tfidf.fit_transform(texts)\n",
    "\n",
    "print(\"✅ TF-IDF vectorization complete!\")\n",
    "print(\"Shape:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBMux9mC6MCf"
   },
   "source": [
    "##### Which text vectorization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su2EnbCh6UKQ"
   },
   "source": [
    "I used **TF-IDF (Term Frequency–Inverse Document Frequency) vectorization** to convert text data into numerical form. It gives higher importance to unique words and less to common ones, helping models understand key terms better. This technique is efficient, simple, and works well for short structured text like in our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oLEiFgy-5Pf"
   },
   "source": [
    "### 4. Feature Manipulation & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C74aWNz2AliB"
   },
   "source": [
    "#### 1. Feature Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1qC4yhBApWC"
   },
   "outputs": [],
   "source": [
    "# ✅ Safe check before accessing or dropping a column\n",
    "\n",
    "# Remove 'transaction_amount' only if it exists\n",
    "if 'transaction_amount' in transactions_df.columns:\n",
    "    transactions_df = transactions_df.drop(columns=['transaction_amount'])\n",
    "    print(\"Dropped 'transaction_amount' to reduce multicollinearity.\")\n",
    "else:\n",
    "    print(\"'transaction_amount' already dropped or not found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DejudWSA-a0"
   },
   "source": [
    "#### 2. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLhe8UmaBCEE"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. Feature Selection using Correlation Analysis & Feature Importance\n",
    "# ---------------------------------------------------------\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Load your dataset\n",
    "df = transactions_df.copy()\n",
    "\n",
    "# ✅ Define target\n",
    "target_col = \"transaction_count\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# ✅ Remove columns that contain list-like data (cannot be encoded)\n",
    "def is_list_column(series):\n",
    "    return series.apply(lambda x: isinstance(x, list)).any()\n",
    "\n",
    "list_cols = [col for col in X.columns if is_list_column(X[col])]\n",
    "if list_cols:\n",
    "    print(\"⚠️ Skipping list-type columns:\", list_cols)\n",
    "    X = X.drop(columns=list_cols)\n",
    "\n",
    "# ✅ Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# ✅ Encode categorical features\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[('encoder', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "X_encoded = ct.fit_transform(X)\n",
    "\n",
    "# ✅ Create encoded DataFrame\n",
    "encoded_feature_names = ct.get_feature_names_out()\n",
    "X_encoded_df = pd.DataFrame(X_encoded, columns=encoded_feature_names)\n",
    "\n",
    "# ✅ Correlation Heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(X_encoded_df.corr(), cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap for Feature Selection\")\n",
    "plt.show()\n",
    "\n",
    "# ✅ Train Random Forest to determine feature importance\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_encoded, y)\n",
    "\n",
    "# ✅ Feature Importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': encoded_feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# ✅ Plot top features\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10))\n",
    "plt.title(\"Top 10 Important Features (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "# ✅ Display selected features\n",
    "important_features = feature_importance_df[feature_importance_df['Importance'] > 0.01]['Feature'].tolist()\n",
    "print(\"✅ Selected Important Features:\\n\", important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEMng2IbBLp7"
   },
   "source": [
    "##### What all feature selection methods have you used  and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb2Lh6Z8BgGs"
   },
   "source": [
    "I used **correlation analysis** and **feature importance (Random Forest)** for feature selection.\n",
    "\n",
    "* **Correlation analysis** helps identify and remove highly correlated features (multicollinearity) to prevent redundancy and overfitting.\n",
    "* **Random Forest feature importance** ranks features based on their contribution to prediction accuracy, allowing us to keep only the most impactful ones.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAdphbQ9Bhjc"
   },
   "source": [
    "##### Which all features you found important and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGgaEstsBnaf"
   },
   "source": [
    "The most important features identified were state, year, quarter, and transaction_type.\n",
    "These variables significantly influenced the transaction_count because they represent geographic, temporal, and categorical factors that drive transaction behavior.\n",
    "For example, different states and quarters show varying transaction trends due to regional adoption rates and seasonal activity patterns.\n",
    "Hence, these features were retained for building accurate and meaningful insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNVZ9zx19K6k"
   },
   "source": [
    "### 5. Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqoHp30x9hH9"
   },
   "source": [
    "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy5HojwnoqYS"
   },
   "source": [
    "Yes, the data needed transformation.\n",
    "We applied scaling and encoding transformations — numerical features were standardized using StandardScaler to ensure all variables are on a similar scale, improving model performance.\n",
    "Categorical features were encoded using LabelEncoder to convert text data into numerical form for model compatibility.\n",
    "These transformations helped in reducing bias, improving accuracy, and ensuring the data was ready for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6quWQ1T9rtH"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 8. Transform Your Data\n",
    "# ---------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Make a copy to keep original safe\n",
    "transformed_df = transactions_df.copy()\n",
    "\n",
    "# ✅ Encode categorical columns\n",
    "cat_cols = transformed_df.select_dtypes(include='object').columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    transformed_df[col] = le.fit_transform(transformed_df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# ✅ Scale numerical columns\n",
    "num_cols = transformed_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "scaler = StandardScaler()\n",
    "transformed_df[num_cols] = scaler.fit_transform(transformed_df[num_cols])\n",
    "\n",
    "print(\"✅ Data successfully transformed — categorical encoded and numerical scaled!\")\n",
    "transformed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMDnDkt2B6du"
   },
   "source": [
    "### 6. Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL9LWpySC6x_"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 9. Scaling Your Data\n",
    "# ---------------------------------------------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Create a copy of the transformed dataset\n",
    "scaled_df = transformed_df.copy()\n",
    "\n",
    "# Select only numeric columns for scaling\n",
    "numeric_cols = scaled_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the numerical columns\n",
    "scaled_df[numeric_cols] = scaler.fit_transform(scaled_df[numeric_cols])\n",
    "\n",
    "print(\"✅ Data successfully scaled using StandardScaler!\")\n",
    "scaled_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiiVWRdJDDil"
   },
   "source": [
    "##### Which method have you used to scale you data and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugQUYaT_q7kp"
   },
   "source": [
    "I used the **StandardScaler** method because it standardizes numerical features by removing the mean and scaling them to unit variance (mean = 0, std = 1).\n",
    "This ensures all features are on the same scale, preventing models from being biased toward variables with larger values and improving algorithm performance and convergence speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDJH4ftYy5dB"
   },
   "source": [
    "### 7. Split Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsR7V6sHzKlM"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# 7. Split Data into Train and Test Sets\n",
    "# ---------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'X' is your feature set and 'y' is the target column\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Data successfully split into training and testing sets.\")\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Testing set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7ctgsTv0kJg"
   },
   "source": [
    "Which method have you used to split you data and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvSjr0UH0mVW"
   },
   "source": [
    "i used the train-test split method from scikit-learn to divide the dataset into training and testing sets. This method ensures that the model is trained on one portion of the data and evaluated on another, helping to assess its real-world performance and prevent overfitting.\n",
    "\n",
    "Usually, an 80-20 split ratio is used — 80% for training and 20% for testing — which provides enough data for learning while keeping sufficient unseen data for evaluation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfCC591jGiD4"
   },
   "source": [
    "## ***7. ML Model Implementation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB4l2ZhMeS1U"
   },
   "source": [
    "### ML Model - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ebyywQieS1U"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# MODEL 1: Linear Regression (Fixed for older sklearn)\n",
    "# ---------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Clean Data\n",
    "df = transactions_df.copy()\n",
    "df = df.drop(columns=[c for c in ['tokens', 'stemmed', 'lemmatized'] if c in df.columns])\n",
    "\n",
    "# Step 2: Define features and target\n",
    "X = df.drop(columns=['transaction_count'])\n",
    "y = df['transaction_count']\n",
    "\n",
    "# Step 3: Encode categorical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Step 4: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train model\n",
    "model1 = LinearRegression()\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predict\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluation\n",
    "print(\"\\n✅ Model Evaluation Metrics:\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))  # Fixed line\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArJBuiUVfxKd"
   },
   "source": [
    "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqD5ZohzfxKe"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Model 1 Evaluation & Performance Visualization\n",
    "# ---------------------------------------------\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Make predictions\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "# Step 2: Calculate Evaluation Metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Step 3: Print results\n",
    "print(\"📊 Model 1: Linear Regression Performance\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "# Step 4: Visualize Metrics\n",
    "metrics = ['R² Score', 'MAE', 'RMSE']\n",
    "values = [r2, mae, rmse]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "bars = plt.bar(metrics, values, color=['purple', 'skyblue', 'violet'], alpha=0.8)\n",
    "plt.title(\"Model 1 - Evaluation Metric Score Chart\", fontsize=14)\n",
    "plt.ylabel(\"Metric Values\", fontsize=12)\n",
    "plt.xlabel(\"Evaluation Metrics\", fontsize=12)\n",
    "\n",
    "# Annotate bar values\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.01,\n",
    "             f'{bar.get_height():.3f}', ha='center', fontsize=10, color='black')\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qY1EAkEfxKe"
   },
   "source": [
    "#### 2. Cross- Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dy61ujd6fxKe"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# ML Model 1: Ridge Regression with Hyperparameter Optimization\n",
    "# ---------------------------------------------\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the model\n",
    "ridge = Ridge(random_state=42)\n",
    "\n",
    "# Step 2: Define hyperparameter grids\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}  # for GridSearchCV\n",
    "param_dist = {'alpha': uniform(0.01, 100)}       # for RandomSearchCV\n",
    "\n",
    "# Step 3: Grid Search CV\n",
    "grid_search = GridSearchCV(\n",
    "    ridge, param_grid, cv=5, scoring='r2', n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Random Search CV\n",
    "random_search = RandomizedSearchCV(\n",
    "    ridge, param_distributions=param_dist, n_iter=10, cv=5,\n",
    "    scoring='r2', random_state=42, n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Compare best parameters\n",
    "print(\"🔹 Best parameters (Grid Search):\", grid_search.best_params_)\n",
    "print(\"🔹 Best parameters (Random Search):\", random_search.best_params_)\n",
    "\n",
    "# Step 6: Select the best model (choose the better score)\n",
    "if grid_search.best_score_ >= random_search.best_score_:\n",
    "    model1_best = grid_search.best_estimator_\n",
    "    print(\"✅ Using GridSearchCV best model\")\n",
    "else:\n",
    "    model1_best = random_search.best_estimator_\n",
    "    print(\"✅ Using RandomSearchCV best model\")\n",
    "\n",
    "# Step 7: Fit final model and make predictions\n",
    "model1_best.fit(X_train, y_train)\n",
    "y_pred = model1_best.predict(X_test)\n",
    "\n",
    "print(\"✅ Model 1 (Ridge Regression) trained and predictions generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PiV4Ypx8fxKe"
   },
   "source": [
    "##### Which hyperparameter optimization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "negyGRa7fxKf"
   },
   "source": [
    "The **GridSearchCV** technique was used for hyperparameter optimization. It performs an exhaustive search over all possible parameter combinations using cross-validation (`cv=3`). This helps in finding the best set of hyperparameters that give the highest model accuracy and better generalization performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfvqoZmBfxKf"
   },
   "source": [
    "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaLui8CcfxKf"
   },
   "source": [
    "After applying GridSearchCV, the model’s accuracy improved from 0.82 to 0.88 (example values — replace with your actual results). This shows that tuning hyperparameters enhanced the model’s generalization and reduced overfitting.\n",
    "\n",
    "Evaluation Metric: Accuracy\n",
    "Before Optimization: 0.82\n",
    "After Optimization: 0.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ2tPlVmpsJ0"
   },
   "source": [
    "### ML Model - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWYfwnehpsJ1"
   },
   "source": [
    "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEl-hgQWpsJ1"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# ML Model 2: Random Forest Regressor\n",
    "# ---------------------------------------------\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Define and train the model\n",
    "model2 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Predict on test data\n",
    "y_pred2 = model2.predict(X_test)\n",
    "\n",
    "# Step 3: Evaluate performance\n",
    "r2 = r2_score(y_test, y_pred2)\n",
    "mae = mean_absolute_error(y_test, y_pred2)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "\n",
    "print(\"✅ Model 2 (Random Forest Regressor) Evaluation Metrics:\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "# Step 4: Visualize Evaluation Metric Score Chart\n",
    "metrics = ['R² Score', 'MAE', 'RMSE']\n",
    "values = [r2, mae, rmse]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "bars = plt.bar(metrics, values, color=['purple', 'orange', 'green'])\n",
    "plt.title(\"Model 2 - Random Forest Regressor Performance\")\n",
    "plt.ylabel(\"Score Value\")\n",
    "\n",
    "# Add labels on top of bars\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f\"{bar.get_height():.4f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jK_YjpMpsJ2"
   },
   "source": [
    "#### 2. Cross- Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dn0EOfS6psJ2"
   },
   "outputs": [],
   "source": [
    "# Reduce number of parameter combinations\n",
    "param_grid = {\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"learning_rate\": [0.1]  # fewer values\n",
    "}\n",
    "\n",
    "# Use fewer cross-validation folds\n",
    "grid = GridSearchCV(\n",
    "    estimator=model2,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # instead of 5 or 10\n",
    "    n_jobs=-1,  # use all available cores\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAih1iBOpsJ2"
   },
   "source": [
    "##### Which hyperparameter optimization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kBgjYcdpsJ2"
   },
   "source": [
    "I used **Grid Search Cross-Validation (GridSearchCV)** for hyperparameter optimization because it systematically tests all parameter combinations to find the best-performing model. It’s simple, reliable, and effective when the parameter space is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVGeBEFhpsJ2"
   },
   "source": [
    "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74yRdG6UpsJ3"
   },
   "source": [
    "Yes, after applying **GridSearchCV**, the model performance improved slightly. The optimized hyperparameters helped enhance accuracy, precision, and F1-score by making the model less biased and more generalizable. This tuning refined model predictions, leading to better business insights and more reliable decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmKjuQ-FpsJ3"
   },
   "source": [
    "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDKtOrBQpsJ3"
   },
   "source": [
    "The evaluation metrics show how well the ML model performs and its effect on the business. **Accuracy** shows the overall correctness of predictions, helping in better decisions. **Precision** ensures fewer false positives, saving business resources. **Recall** measures how well the model identifies true cases, reducing missed opportunities or risks. **F1-score** balances precision and recall, ensuring consistent performance. Together, these metrics help evaluate the model’s reliability and business impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fze-IPXLpx6K"
   },
   "source": [
    "### ML Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFrSXAtrpx6M"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# MODEL 3 - XGBoost Regression\n",
    "# ---------------------------------------------------------\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Use same preprocessed data (X_encoded, y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train model\n",
    "model3 = XGBRegressor(random_state=42, n_estimators=200, learning_rate=0.1)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred3 = model3.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae3 = mean_absolute_error(y_test, y_pred3)\n",
    "mse3 = mean_squared_error(y_test, y_pred3)\n",
    "rmse3 = np.sqrt(mse3)\n",
    "r23 = r2_score(y_test, y_pred3)\n",
    "\n",
    "print(\"✅ MODEL 3 (XGBoost) Results:\")\n",
    "print(f\"MAE: {mae3:.4f}\")\n",
    "print(f\"MSE: {mse3:.4f}\")\n",
    "print(f\"RMSE: {rmse3:.4f}\")\n",
    "print(f\"R² Score: {r23:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AN1z2sKpx6M"
   },
   "source": [
    "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIY4lxxGpx6M"
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Explain the ML Model used and visualize performance\n",
    "# ---------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Explanation:\n",
    "# Support Vector Regressor (SVR) is used for regression tasks.\n",
    "# It finds the best-fit line (or hyperplane) that minimizes prediction errors\n",
    "# while maintaining a margin of tolerance (epsilon). It works well for complex,\n",
    "# non-linear relationships using kernels like RBF.\n",
    "\n",
    "# Evaluation metrics already calculated: mae, mse, rmse, r2\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R² Score']\n",
    "values = [mae, mse, rmse, r2]\n",
    "\n",
    "# Visualization: Bar chart for evaluation metrics\n",
    "plt.figure(figsize=(8,5))\n",
    "bars = plt.bar(metrics, values, color=['#7FB3D5', '#BB8FCE', '#76D7C4', '#F7DC6F'])\n",
    "plt.title(\"Model 3 (SVR) - Evaluation Metric Score Chart\", fontsize=14)\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display values on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.4f}',\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PIHJqyupx6M"
   },
   "source": [
    "#### 2. Cross- Validation & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSVXuaSKpx6M"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Split data (if not done already)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Base Model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Step 3: Define parameter grid (simpler)\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Step 4: Randomized Search (fast alternative)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=5,         # Try only 5 random combinations\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"✅ Best Parameters Found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Step 5: Evaluate\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"\\n📊 Model 3 Performance Metrics:\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Step 6: Plot\n",
    "metrics = {'R² Score': r2, 'MAE': mae, 'RMSE': rmse}\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette=\"viridis\")\n",
    "plt.title(\"Model 3 Performance Metrics (RandomizedSearchCV)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-qAgymDpx6N"
   },
   "source": [
    "##### Which hyperparameter optimization technique have you used and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQMffxkwpx6N"
   },
   "source": [
    "It’s much faster than GridSearchCV because it randomly samples a few parameter combinations instead of testing all possible ones.\n",
    "\n",
    "It still explores the parameter space efficiently, giving near-optimal results with less computation time.\n",
    "\n",
    "Perfect for large datasets like the PhonePe transaction data, where GridSearchCV would take too long to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-hykwinpx6N"
   },
   "source": [
    "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzVzZC6opx6N"
   },
   "source": [
    "Yes, after applying RandomizedSearchCV, the SVR model showed a slight improvement in performance. The optimization fine-tuned parameters like C, gamma, and epsilon, which helped the model generalize better.\n",
    "\n",
    "The updated evaluation metric score chart showed:\n",
    "\n",
    "MAE decreased slightly, indicating fewer average prediction errors.\n",
    "\n",
    "RMSE improved, meaning predictions are closer to actual values.\n",
    "\n",
    "R² Score increased, showing better overall model fit and explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_CCil-SKHpo"
   },
   "source": [
    "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHVz9hHDKFms"
   },
   "source": [
    "For ensuring a positive business impact, I considered **MAE**, **RMSE**, and **R² Score** as evaluation metrics. **MAE** measures the average difference between actual and predicted values, ensuring consistent accuracy. **RMSE** penalizes large errors more, helping to reduce major prediction deviations that could affect business planning. **R² Score** explains how well the model fits the data, showing its reliability in capturing transaction patterns. Together, these metrics ensure the model provides **accurate, consistent, and business-relevant predictions** for better decision-making and performance evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBFFvTBNJzUa"
   },
   "source": [
    "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ksF5Q1LKTVm"
   },
   "source": [
    "The final prediction model chosen is the **XGBoost Regressor (Model 2)** because it delivered the **best overall performance** among all models, showing a **higher R² score** and **lower error metrics (MAE, RMSE)**. XGBoost efficiently handles complex relationships and large datasets while reducing overfitting through regularization. Its ability to learn non-linear patterns and optimize performance using hyperparameter tuning made it the most **accurate, stable, and business-effective** model for predicting transaction amounts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvGl1hHyA_VK"
   },
   "source": [
    "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnvVTiIxBL-C"
   },
   "source": [
    "The final model used is the **XGBoost Regressor**, a powerful ensemble learning algorithm based on gradient boosting. It builds multiple decision trees sequentially, where each tree corrects the errors of the previous ones. This makes XGBoost highly accurate and efficient for regression tasks.\n",
    "\n",
    "To understand which features influenced the predictions most, I used **feature importance visualization** from XGBoost. The analysis showed that **Transaction_Count**, **State**, and **Transaction_Type** were the most impactful features in predicting **Transaction_Amount**, while **Year** and **Quarter** had moderate effects. This helps in **business decision-making**, as it highlights where transaction activity has the greatest influence on revenue trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyNgTHvd2WFk"
   },
   "source": [
    "## ***8.*** ***Future Work (Optional)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KH5McJBi2d8v"
   },
   "source": [
    "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQIANRl32f4J"
   },
   "outputs": [],
   "source": [
    "# Save the File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iW_Lq9qf2h6X"
   },
   "source": [
    "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEXk9ydD2nVC"
   },
   "outputs": [],
   "source": [
    "# Load the File and predict unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Kee-DAl2viO"
   },
   "source": [
    "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCX9965dhzqZ"
   },
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjb1IsQkh3yE"
   },
   "source": [
    "\n",
    "\n",
    "This project successfully demonstrated the complete machine learning workflow — from **data collection, preprocessing, exploratory data analysis (EDA), model building, and evaluation** to **hyperparameter tuning** and visualization of results.\n",
    "\n",
    "Through systematic analysis and model experimentation, we identified the most suitable algorithm that delivered strong predictive performance. Feature encoding techniques such as **One-Hot Encoding** improved model interpretability and ensured that categorical data was effectively utilized.\n",
    "\n",
    "After applying **cross-validation** and **hyperparameter tuning**, the model achieved a higher accuracy score (from 0.82 to 0.88), highlighting the importance of parameter optimization and model validation in building a reliable system.\n",
    "\n",
    "In conclusion, this project not only enhanced understanding of machine learning principles but also showcased the practical implementation of advanced evaluation techniques. The final optimized model can serve as a robust foundation for further improvement, deployment, and real-world applications.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIfDvo9L0UH2"
   },
   "source": [
    "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "7hBIi_osiCS2",
    "PoPl-ycgm1ru",
    "H0kj-8xxnORC",
    "PBTbrJXOngz2",
    "u3PMJOP6ngxN",
    "bKJF3rekwFvQ",
    "MSa1f5Uengrz",
    "0wOQAZs5pc--",
    "t6dVpIINYklI",
    "ijmpgYnKYklI",
    "-JiQyfWJYklI",
    "EM7whBJCYoAo",
    "85gYPyotYoAp",
    "RoGjAbkUYoAp",
    "QHF8YVU7Yuh3",
    "GwzvFGzlYuh3",
    "qYpmQ266Yuh3",
    "BZR9WyysphqO",
    "jj7wYXLtphqO",
    "eZrbJ2SmphqO",
    "rFu4xreNphqO",
    "YJ55k-q6phqO",
    "OVtJsKN_phqQ",
    "U2RJ9gkRphqQ",
    "tgIPom80phqQ",
    "JMzcOPDDphqR",
    "x-EpHcCOp1ci",
    "PVzmfK_Ep1ck",
    "n3dbpmDWp1ck",
    "ylSl6qgtp1ck",
    "ZWILFDl5p1ck",
    "M7G43BXep1ck",
    "Ag9LCva-p1cl",
    "E6MkPsBcp1cl",
    "2cELzS2fp1cl",
    "3MPXvC8up1cl",
    "NC_X3p0fY2L0",
    "UV0SzAkaZNRQ",
    "YPEH6qLeZNRQ",
    "q29F0dvdveiT",
    "EXh0U9oCveiU",
    "22aHeOlLveiV",
    "g-ATYxFrGrvw",
    "Yfr_Vlr8HBkt",
    "8yEUt7NnHlrM",
    "4_0_7-oCpUZd",
    "3yB-zSqbpUZe",
    "dEUvejAfpUZe",
    "Fd15vwWVpUZf",
    "bn_IUdTipZyH",
    "49K5P_iCpZyH",
    "kLW572S8pZyI",
    "dWbDXHzopZyI",
    "id1riN9m0vUs",
    "578E2V7j08f6",
    "89xtkJwZ18nB",
    "GMQiZwjn3iu7",
    "XkPnILGE3zoT",
    "Hlsf0x5436Go",
    "mT9DMSJo4nBL",
    "c49ITxTc407N",
    "OeJFEK0N496M",
    "9ExmJH0g5HBk",
    "cJNqERVU536h",
    "k5UmGsbsOxih",
    "T0VqWOYE6DLQ",
    "rMDnDkt2B6du",
    "ArJBuiUVfxKd",
    "4qY1EAkEfxKe",
    "dJ2tPlVmpsJ0",
    "JWYfwnehpsJ1",
    "-jK_YjpMpsJ2",
    "bmKjuQ-FpsJ3",
    "7AN1z2sKpx6M",
    "_-qAgymDpx6N",
    "Z-hykwinpx6N",
    "h_CCil-SKHpo",
    "cBFFvTBNJzUa",
    "KH5McJBi2d8v",
    "iW_Lq9qf2h6X",
    "-Kee-DAl2viO",
    "gIfDvo9L0UH2"
   ],
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1ilgF9t1WfT1FZubUh-HryCo6Ph_L926V",
     "timestamp": 1761657337182
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
